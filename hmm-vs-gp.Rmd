---
title: "Bayesian Modelling of Gaussian Processes and Hidden Markov Models with Stan"
output:
  html_document:
    toc: yes
    self_contained: no
---

TODO: intro

## Setup 

Let's set up our computing environment:

```{r}
setwd("~/Code/hmm-vs-gp")

# Packages
library(rstan)
library(ggplot2)
library(bayesplot)
library(invgamma)
library(bridgesampling)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
seed = 723456

# Colors
c_light <- c("#DCBCBC")
c_mid <- c("#B97C7C")
c_dark <- c("#8F2727")
c_blue_light <- c("#b2c4df")
c_blue_mid <- c("#6689bf")
c_blue_dark <- c("#3d5272")
color_scheme_set("red")
```


## Gaussian Processes

TODO: intro to GPs, why we want to use them, assume slowly moving, etc

### The Math

TODO: intro/math to GPs, but we have values between 0 and 1, not otherwise, so we need to use the logit link function, and therefore we're going to be thinking of our data as coming from a multivariate logistic-normal distribution (but with *sigmoidal* elements, _not_ a simplex, i.e. all the elements in y don't sum to 1) and so mathematically it's the same as logit-ing your x values, sampling from a normal distribution in infinity space w/ those transformed values, and then applying the scale

The logit function is the inverse of the sigmoidal (logistic) function.

$$
\text{logit}(x) = \log \left( \frac{x}{1-x} \right)
$$

A multivariate logistic normal distribution is defined between 0 and 1 in each of $N$ dimensions,

$$
\{ \mathbf{x} \in \mathbb{R}^N ~ | ~ \forall x_i, ~ 0 < x_i < 1 \}
$$

which is the space our data is in!  That is, we have $N$ datapoints (which we treat as a single datapoint in $N$-dimensional space), each of which falls between 0 and 1.  The probability density function for the multivariate logistic normal distribution is

$$
\mathcal{N}_{\text{logit}}(\mathbf{x};\mathbf{\mu},\mathbf{\Sigma}) = 
  \frac{1}{\prod_{i=1}^D x_i (1-x_i)} 
  \frac{1}{\sqrt{|2 \pi \Sigma |}}
  \exp \left( 
    -\frac{1}{2} 
    \left( \text{logit}(\mathbf{x})-\mathbf{\mu} \right)^\top
    \mathbf{\Sigma}^{-1}
    \left( \text{logit}(\mathbf{x})-\mathbf{\mu} \right)
  \right)
$$

Which is equivalent to the value of a (regular) multivariate normal distribution given logit-transformed $x$, times a scaling factor which ensures the density integrates to 1.

$$
\mathcal{N}_{\text{logit}}(\mathbf{x};\mathbf{\mu},\mathbf{\Sigma}) = 
  \frac{1}{\prod_{i=1}^D x_i (1-x_i)} 
  \mathcal{N}(\text{logit}(\mathbf{x});\mathbf{\mu},\mathbf{\Sigma})
$$

Without this scaling factor, we would be logit-transforming the $x$ values without appropriately scaling the density to compensate, and the probability density wouldn't integrate to 1!

### Priors

TODO: this is a Bayesian analysis, so... priors!

For the length scale parameter ($\rho$) we use an inverse gamma prior with $\alpha=2$ and $\beta=0.5$.

$$
\frac{1}{\rho} \sim \text{Gamma}(\alpha=2, \beta=0.5)
$$

This results in a distribution which is defined $>0$, and has a low density near zero and at higher values.

```{r, dev='svg'}
rho_prior <- function(x) dinvgamma(x, 2, 1/2)
plot(rho_prior, 0, 3, n=300, xlab='Rho', ylab='',
     main="Length scale parameter prior",
     yaxt='n', bty='n', lwd=3, col=c_blue_mid)
```

For the signal standard deviation parameter ($\alpha$, aka "marginal" standard deviation or "ouput" standard deviation) we use a half-normal prior (a normal distribution but with zero density $<0$) with $\sigma=2$.

$$
\alpha \sim \mathcal{N}(\mu=0, \sigma=2), ~ \alpha \geq 0
$$

This results in a distribution which only has density $\geq 0$, and has a low density at higher values.

```{r, dev='svg'}
alpha_prior <- function(x) ifelse(x>=0, 2*dnorm(x, 0, 2), 0)
plot(alpha_prior, 0, 6, n=300, xlab='Alpha', ylab='',
     main="Signal standard deviation parameter prior",
      yaxt='n', bty='n', lwd=3, col=c_blue_mid)
```

For the noise standard deviation parameter ($\sigma$) we also use a half-normal prior, but with $\sigma=1$.

$$
\sigma \sim \mathcal{N}(\mu=0, \sigma=1), ~ \alpha \geq 0
$$

This also results in a distribution which only has density $\geq 0$, and has a low density at higher values.

```{r, dev='svg'}
sigma_prior <- function(x) ifelse(x>=0, 2*dnorm(x, 0, 1), 0)
plot(sigma_prior, 0, 6, n=300, xlab='Sigma', ylab='',
     main="Noise standard deviation parameter prior",
      yaxt='n', bty='n', lwd=3, col=c_blue_mid)
```

### Generating Data from a Gaussian Process

TODO: talk about stan file to generate data

```{r}
writeLines(readLines("simulate_lgp.stan"))
```

We can generate data from a Gaussian process using this Stan model.  Let's generate 100 samples in the range 0 to 5.

```{r}
# Data
N = 100
x = seq(0, 5, l=N)
```

We will also set the hyperparameters of the gaussian process.

```{r}
# Parameters
rho = 0.5
alpha = 2
sigma = 0.3
```

Now we can run the Stan model, which will generate data from a Gaussian process with these parameters.

```{r}
# Simulate
sim_params = list(N=N, x=x, rho=rho, alpha=alpha, sigma=sigma)
sim_gp = stan(file='simulate_lgp.stan', data=sim_params, iter=1, 
              chains=1, seed=seed, algorithm="Fixed_param")

# Extract simulated output
f = extract(sim_gp)$f[1,]
y = extract(sim_gp)$y[1,]
sim_data = data.frame(x=x, y=y, f=f)
```

Let's plot our generated Gaussian process:

```{r, dev='svg'}
# Plot generated data
ggplot(sim_data) +
  geom_line(aes(x=x, y=f), size=1) +
  geom_point(aes(x=x, y=y), color=c_mid) +
  ggtitle('Simulated Gaussian Process')
```

The line above is the latent function which was generated, and the dots are the generated observations.  Notice how the latent function varies smoothly over time, and varies between 0 and 1.  The observations have more jitter than the latent function (because they are generated from that function but with added noise), but each point is in the neigborhood of the previous point, and they are also bounded between 0 and 1.

### Fitting a Gaussian Process

Now that we've generated some data from a Gaussian process, we can attempt to fit a Gaussian process model to that data.  We'll use a separate Stan model, one which fits the parameters of the model to the data, instead of generating data from the parameters.

```{r}
writeLines(readLines("lgp.stan"))
```

Let's fit the model to our generated data.

```{r}
# Fit
stan_rdump(c("N", "x", "y"), file="simulated_lgp.data.R")
gp_data = read_rdump("simulated_lgp.data.R")
fit_gp = stan(file='lgp.stan', data=gp_data, seed=seed)
```

After MCMC sampling has finished, we can view the posterior intervals for each parameter.

```{r}
# Show results of fit
print(fit_gp)
```

Before checking that our fits recover the true parameters which were used to generate the data, we'll run a few diagnostics to ensure that the MCMC sampling went smoothly and that our model isn't misspecified.  

First, we'll check that the MCMC chains converged.  The `Rhat` value in the table above should be close to 1.  The posterior distributions should also look the same across chains.

```{r, dev='svg'}
# Plot density per chain
posterior = as.array(fit_gp)
mcmc_dens_overlay(posterior, pars=c("rho", "alpha", "sigma"))
```

The distributions look similar between chains, which is good!  This should also be apparent in the trace plot (the plot of posterior sample values vs sample number), and there shouldn't be any obvious "sliding" over time.

```{r, dev='svg'}
# Plot trace per chain
mcmc_trace(posterior, pars=c("rho", "alpha", "sigma"))
```

Again, everything looks good here.

We should also check that none of the iterations hit the max tree depth.

```{r}
# Check no chains hit max tree depth
check_treedepth(fit_gp)
```

Another diagnostic to check is the energy Bayesian fraction of missing information (E-BFMI).

```{r}
# Check the energy Bayesian Fraction of Missing Information
check_energy(fit_gp)
```

Finally, we should check that iterations did not end with a divergence, indicating areas of parameter space that the sampling was not able to explore well.

```{r}
# Check for divergences
check_divergences(fit_gp)
```

All our diagnostics seem to indicate that we were able to fit the data without any sampling issues!  Now we can see how the posterior distributions for the parameters match up to the true parameters we used to generate the data.  First let's define a function which will plot our posterior distributions from the MCMC samples and their 95% confidence intervals.

```{r}
# Function to plot posterior distributions w/ 95% confidence intervals
interval_density = function(x, bot=0.025, top=0.975,
                            main="", xlab="", ylab="",
                            xlim=c(min(x),max(x)), lwd=1,
                            col1=c("#DCBCBC"), col2=c("#B97C7C")) {
  dens = density(x)
  plot(dens, main=main, xlab=xlab, ylab=ylab, xlim=xlim,
       lwd=lwd, yaxt='n', bty='n', type='n')
  polygon(dens, col=col1, border=NA)
  qbot <- quantile(x, bot)
  qtop <- quantile(x, top)
  x1 = min(which(dens$x >= qbot))
  x2 = max(which(dens$x < qtop))
  with(dens, polygon(x=x[c(x1,x1:x2,x2)], y=c(0, y[x1:x2], 0), col=col2, border=NA))
}
```

And now we can view the posteriors as compared to the true parameter values which were used to generate the data.

```{r, dev='svg'}
# Plot true vs posterior
posterior = extract(fit_gp)
par(mfrow=c(1, 3))

interval_density(posterior$rho, xlab="rho")
abline(v=rho, col=c_dark, lwd=3)

interval_density(posterior$alpha, xlab="alpha")
abline(v=alpha, col=c_dark, lwd=3)

interval_density(posterior$sigma, xlab="sigma")
abline(v=sigma, col=c_dark, lwd=3)
```

The true parameter values are all within the bulk of the posterior, meaning we were able to successfully recover the true parameters!

## Hidden Markov Models

TODO: intro

### The Math

Our model will have two hidden states ($N=2$).  This means that our model has two free parameters which control the transition probabilities $\phi$.  The probability of transitioning from state $i$ to state $j$ is $\phi_{i,j}$.  In our Stan model, we'll fit the "recurrent" transition probabilities - that is, the probability that the hidden state at the current timestep will be the same as the last timestep.  So, the two free parameters which control the transition probabilities are:

$$
\phi_{i,i} ~ \text{for} ~ i \in \{1,2\}
$$

We model the probability of an observation given the state as a beta distribution.  However, for each hidden state, one parameter of the distribution is fixed at $1$, and the other parameter is free.  This gives us two additional parameters ($\theta_1$ and $\theta_2$) which control the observation probabilities corresponding to the hidden states.  So for the first state $x_1$,


$$
y|x_1 \sim \text{Beta}(1, \theta_1)
$$

and for the second state $x_2$,

$$
y|x_2 \sim \text{Beta}(\theta_2, 1)
$$

TODO: explain why and plot of the beta dist for each state

TODO: explain how you compute the prob marginalized over hidden states w/ the forward algorithm

### Priors

For the observation model parameters ($\theta$) we use a gamma prior with $\alpha,\beta=2$, and limit the values to be $\geq 1$,

$$
\theta_i - 1 \sim \text{Gamma}(\alpha=2, \beta=2)
$$

This results in a distribution which has zero density $<1$, and low density at higher values.

NOTE: could also use a log-Cauchy distribution ($\theta \sim 1/(log(x)^2+1)$)?  Same bounds, has ridiculously long tail on the positive end...

```{r, dev='svg'}
theta_prior <- function(x) dgamma(x-1, 2, 1/2)
plot(theta_prior, 0, 10, n=300, xlab='Theta', ylab='',
     main="Observation model parameter prior",
      yaxt='n', bty='n', lwd=3, col=c_blue_mid)
```

(Side note: the notation above and the Stan programs below use the $\text{Gamma}(\alpha,\beta)$ parameterization of the gamma distribution, while the R function `dgamma` uses the $\text{Gamma}(\kappa,\theta)$ parameterization, where $\kappa=\alpha$ and $\theta=1/\beta$, in case you were wondering why the parameters in the call to `dgamma` were $2,1/2$ instead of $2,2$)

For the recurrent transition probability prior, we use a beta distribution with $\alpha,\beta=2$,

$$
\phi_{i,i} \sim \text{Beta}(2, 2)
$$

This results in a distribution which is bounded between $0$ and $1$, and has lower density towards more extreme values.

```{r, dev='svg'}
trans_prior <- function(x) dbeta(x, 2, 2)
plot(trans_prior, 0, 1, n=300, xlab='Phi_{i,i}', ylab='',
     main="Recurrent transition probability prior",
     type='l', yaxt='n', bty='n', lwd=3, col=c_blue_mid)
```


### Generating Data from a Hidden Markov Model

TODO: talk about stan file to generate data

```{r}
writeLines(readLines("simulate_hmm.stan"))
```

We can use this Stan model to generate data from our hidden Markov model.  Let's generate 100 samples.

```{r}
# Data
N = 100
```

We will also set the parameters of the HMM.

```{r}
# Parameters
phi = array(c(0.8, 0.2, 0.2, 0.8), dim=c(2,2)) #transition probabilities
theta = c(5, 5) #observation distribution parameters
```

Now we can run the Stan model, which will generate data from a hidden Markov model with these parameters.

```{r}
# Simulate
sim_params = list(N=N, phi=phi, theta=theta)
sim_hmm = stan(file='simulate_hmm.stan', data=sim_params, iter=1, 
               chains=1, seed=seed, algorithm="Fixed_param")

# Extract simulated output
s = t(extract(sim_hmm)$s)-1
y = t(extract(sim_hmm)$y)
sim_data = data.frame(x=seq(N), s=s, y=y)
```

Let's plot our generated Gaussian process:

```{r, dev='svg'}
# Plot generated data
ggplot(sim_data) +
  geom_line(aes(x=x, y=s), size=1) +
  geom_point(aes(x=x, y=y), color=c_mid) +
  ggtitle('Simulated Hidden Markov Model')
```

Notice how the latent function varies smoothly over time, and varies between 0 and 1.  The observations have more jitter than the latent function (because they are generated from that function but with added noise), but each point is in the neigborhood of the previous point, and they are also bounded between 0 and 1.  Because of the way we defined the observation part of the model, observations during the first hidden state are close to 0, and observations during the second hidden state are close to 1.  However, all observations are between 0 and 1, and there is again some noise to the observations.


### Fitting a Hidden Markov Model

Now that we've generated some data from a Gaussian process, we can attempt to fit a Gaussian process model to that data.  We'll use a separate Stan model, one which fits the parameters of the model to the data, instead of generating data from the parameters.

```{r}
writeLines(readLines("hmm.stan"))
```

Let's fit the model to our generated data.

```{r}
# Fit
y = y[,1]
stan_rdump(c("N", "y"), file="simulated_hmm.data.R")
hmm_data = read_rdump("simulated_hmm.data.R")
fit_hmm = stan(file='hmm.stan', data=hmm_data, seed=seed)
```

After MCMC sampling has finished, we can view the posterior intervals for each parameter.

```{r}
# Show results of fit
print(fit_hmm)
```

Again we'll check several diagnoistics to ensure our sampling effectively explored the parameter space.  The Rhat values look good, and if we look at the posterior distributions per chain, it appears the chains converged.


```{r, dev='svg'}
# Plot density per chain
posterior = as.array(fit_hmm)
mcmc_dens_overlay(posterior, 
                  pars=c("phi[1,1]", "phi[2,2]", "theta[1]", "theta[2]"))
```

The trace plots also indicate that the chains (likely) explored similar areas of parameter space, and that the warmup period was (probably) long enough.

```{r, dev='svg'}
# Plot trace per chain
mcmc_trace(posterior, 
           pars=c("phi[1,1]", "phi[2,2]", "theta[1]", "theta[2]"))
```

Our other three diagnostics also look good:

```{r}
# Check other diagnostics
check_treedepth(fit_hmm)
check_energy(fit_hmm)
check_divergences(fit_hmm)
```

Now we can see how the posterior distributions for the parameters match up to the true parameters we used to generate the data.  We'll also plot the prior distributions for each parameter so we can see how that effects the posterior.  The prior distribution is plotted in blue, the posterior distribution in red, and the parameter value used to generate the data as a dark red vertical line.

```{r, dev='svg'}
# Plot true vs posterior
posterior = extract(fit_hmm)
par(mfrow=c(2, 2))

beta_prior <- function(x) dbeta(x, 2, 2)
gamma_prior <- function(x) dgamma(x, 2, 1/2)

interval_density(posterior$phi[,1,1], xlab="phi[,1,1]", xlim=c(0,1))
plot(beta_prior, 0, 1, add=TRUE, col=c_blue_light, lwd=3)
abline(v=phi[1,1], col=c_dark, lwd=3)

interval_density(posterior$phi[,2,2], xlab="phi[,2,2]", xlim=c(0,1))
plot(beta_prior, 0, 1, add=TRUE, col=c_blue_light, lwd=3)
abline(v=phi[2,2], col=c_dark, lwd=3)

interval_density(posterior$theta[,1], xlab="theta[1]", xlim=c(0,12))
plot(gamma_prior, 0, 12, add=TRUE, col=c_blue_light, lwd=3)
abline(v=theta[1], col=c_dark, lwd=3)

interval_density(posterior$theta[,2], xlab="theta[2]", xlim=c(0,12))
plot(gamma_prior, 0, 12, add=TRUE, col=c_blue_light, lwd=3)
abline(v=theta[2], col=c_dark, lwd=3)
```

We were mostly able to recover the parameters which generated the data - but not perfectly!  This is because we set parameter values which didn't align very well with the priors - with more data the posteriors would be pulled even closer to the true vaules.  Notice how the posteriors for the first state's parameters ($\phi_{1,1}$ and $\theta_1$) are closer to the true value than the posteriors for the second state's parameters.  This is because in our generated data, more observations occurred during the first state.  Therefore, the fit was able to better infer the true parameters corresponding to that state.  Our posteriors were indeed pulled towards the true parameter values, which is what we want to see, and indicates our Bayesian fit of the hidden Markov model was successful - we'd just ideally have more data!


## Model Comparison using Bridge Sampling 

TODO: intro to why we want to compare which model is more likely, and that we can use bridge sampling to do that

### Bridge Sampling

TODO: math etc of bridge sampling and that it gets you an estimate of the marginal likelihood, and Bayes factor

### Comparing GP and HMM models with Bridge Sampling

Let's use bridge sampling to compare the marginal likelihoods of each model given the data that we generated using a Gaussian process.  If everything is working correctly, the Bayes factor should favor the Gaussian process model over the hidden Markov model (because we used the Gaussian process to generate the data in the first place!).

First we have to fit the hidden Markov model to the data generated by a Gaussian process (we've already fit the GP model to the GP-generated data, so we won't re-do that here).

```{r}
# Fit HMM to GP-generated data
gp_data = read_rdump("simulated_lgp.data.R")
fit_hmm_to_gp = stan(file='hmm.stan', data=gp_data, seed=seed)
```

We are able to fit the hidden Markov model to the data generated by the Gaussian process without problems.

```{r}
# Check MCMC diagnostics
check_treedepth(fit_hmm_to_gp)
check_energy(fit_hmm_to_gp)
check_divergences(fit_hmm_to_gp)
```

The resulting fit looks reasonable.

```{r}
print(fit_hmm_to_gp)
```

```{r, dev='svg'}
# Plot true vs posterior
posterior = extract(fit_hmm_to_gp)
par(mfrow=c(2, 2))
interval_density(posterior$phi[,1,1], xlab="phi[,1,1]")
interval_density(posterior$phi[,2,2], xlab="phi[,2,2]")
interval_density(posterior$theta[,1], xlab="theta[1]")
interval_density(posterior$theta[,2], xlab="theta[2]")
```

However, with data generated by the Gaussian process, the Bayes factor favors the Gaussian process over the hidden Markov model!

```{r}
# Perform bridge sampling and view Bayes factor for GP-generated data
bridge_gp_gp = bridge_sampler(fit_gp)
bridge_hmm_gp = bridge_sampler(fit_hmm_to_gp)
print("Bayes factor of GP to HMM on GP-generated data")
bf(bridge_gp_gp, bridge_hmm_gp)
```

Similarly, we want to ensure that the Bayes factor favors the hidden Markov model over the Gaussian process when fitting to the data generated by the hidden Markov model.

```{r}
# Fit GP to HMM-generated data
hmm_data = read_rdump("simulated_hmm.data.R")
fit_gp_to_hmm = stan(file='lgp.stan', data=hmm_data, seed=seed)
```

We are able to fit the Gaussian process model to the data generated by the hidden Markov model without problems.

```{r}
# Check MCMC diagnostics
check_treedepth(fit_gp_to_hmm)
check_energy(fit_gp_to_hmm)
check_divergences(fit_gp_to_hmm)
```

And the resulting fit looks reasonable.

```{r}
print(fit_gp_to_hmm)
```

```{r, dev='svg'}
# Plot true vs posterior
posterior = extract(fit_gp_to_hmm)
par(mfrow=c(1, 3))
interval_density(posterior$rho, xlab="rho")
interval_density(posterior$alpha, xlab="alpha")
interval_density(posterior$sigma, xlab="sigma")
```

However, with data generated by the hidden Markov model, the Bayes factor favors the hidden Markov model over the Gaussian process!

```{r}
# Perform bridge sampling and view Bayes factor for HMM-generated data
bridge_hmm_hmm = bridge_sampler(fit_hmm)
bridge_gp_hmm = bridge_sampler(fit_gp_to_hmm)
print("Bayes factor of HMM to GP on HMM-generated data")
bf(bridge_hmm_hmm, bridge_gp_hmm)
```

TODO: one weirdness is that the HMM's marginal likelihood is greater when fitting to the GP-generated data than when fitting to the HMM-generated data? That seems wrong...
