---
title: "Bayesian Modelling of Gaussian Processes and Hidden Markov Models with Stan"
output: html_notebook
---

TODO: intro

## Setup 

Let's set up our computing environment:

```{r}
setwd("~/Code/hmm-vs-gp")

# Packages
library(rstan)
library(ggplot2)
library(bayesplot)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
seed = 723456

# Colors
c_light <- c("#DCBCBC")
c_mid <- c("#B97C7C")
c_dark <- c("#8F2727")
c_blue_light <- c("#b2c4df")
c_blue_mid <- c("#6689bf")
c_blue_dark <- c("#3d5272")
color_scheme_set("red")
```


## Gaussian Processes

TODO: intro to GPs, why we want to use them, assume slowly moving, etc

### The Math

TODO: intro/math to GPs, and the logit link function

### Priors

TODO: the priors

TODO: visualize the priors

### Generating Data from a Gaussian Process

TODO: talk about stan file to generate data

```{r}
writeLines(readLines("simulate_lgp.stan"))
```

We can generate data from a Gaussian process using this Stan model.  Let's generate 100 samples in the range 0 to 5.

```{r}
# Data
N = 100
x = seq(0, 5, l=N)
```

We will also set the hyperparameters of the gaussian process.

```{r}
# Parameters
rho = 0.5
alpha = 2
sigma = 0.3
```

Now we can run the Stan model, which will generate data from a Gaussian process with these parameters.

```{r}
# Simulate
sim_params = list(N=N, x=x, rho=rho, alpha=alpha, sigma=sigma)
sim_fit = stan(file='simulate_lgp.stan', data=sim_params, iter=1, 
               chains=1, seed=seed, algorithm="Fixed_param")

# Extract simulated output
f = extract(sim_fit)$f[1,]
y = extract(sim_fit)$y[1,]
sim_data = data.frame(x=x, y=y, f=f)
```

Let's plot our generated Gaussian process:

```{r, dev='svg'}
# Plot generated data
ggplot(sim_data) +
  geom_line(aes(x=x, y=f), size=1) +
  geom_point(aes(x=x, y=y), color=c_mid) +
  ggtitle('Simulated Gaussian Process')
```

The line above is the latent function which was generated, and the dots are the generated observations.  Notice how the latent function varies smoothly over time, and varies between 0 and 1.  The observations have more jitter than the latent function (because they are generated from that function but with added noise), but each point is in the neigborhood of the previous point, and they are also bounded between 0 and 1.

### Fitting a Gaussian Process

Now that we've generated some data from a Gaussian process, we can attempt to fit a Gaussian process model to that data.  We'll use a separate Stan model, one which fits the parameters of the model to the data, instead of generating data from the parameters.

```{r}
writeLines(readLines("lgp.stan"))
```

Let's fit the model to our generated data.

```{r}
# Fit
stan_rdump(c("N", "x", "y"), file="simulated_lgp.data.R")
fit_data = read_rdump("simulated_lgp.data.R")
fit = stan(file='lgp.stan', data=fit_data, seed=seed)
```

After MCMC sampling has finished, we can view the posterior intervals for each parameter.

```{r}
# Show results of fit
print(fit)
```

Before checking that our fits recover the true parameters which were used to generate the data, we'll run a few diagnostics to ensure that the MCMC sampling went smoothly and that our model isn't misspecified.  

First, we'll check that the MCMC chains converged.  The `Rhat` value in the table above should be close to 1.  The posterior distributions should also look the same across chains.

```{r}
# Plot density per chain
posterior = as.array(fit)
mcmc_dens_overlay(posterior, pars=c("rho", "alpha", "sigma"))
```

The distributions look similar between chains, which is good!  This should also be apparent in the trace plot (the plot of posterior sample values vs sample number), and there shouldn't be any obvious "sliding" over time.

```{r}
# Plot trace per chain
mcmc_trace(posterior, pars=c("rho", "alpha", "sigma"))
```

Again, everything looks good here.

We should also check that none of the iterations hit the max tree depth.

```{r}
# Check no chains hit max tree depth
check_treedepth(fit)
```

Another diagnostic to check is the energy Bayesian fraction of missing information (E-BFMI).

```{r}
# Check the energy Bayesian Fraction of Missing Information
check_energy(fit)
```

Finally, we should check that iterations did not end with a divergence, indicating areas of parameter space that the sampling was not able to explore well.

```{r}
# Check for divergences
check_divergences(fit)
```

All our diagnostics seem to indicate that we were able to fit the data without any sampling issues!  Now we can see how the posterior distributions for the parameters match up to the true parameters we used to generate the data.  First let's define a function which will plot our posterior distributions from the MCMC samples and their 95% confidence intervals.

```{r}
# Function to plot posterior distributions w/ 95% confidence intervals
interval_density = function(x, bot=0.025, top=0.975,
                            main="", xlab="", ylab="",
                            xlim=c(min(x),max(x)), lwd=1,
                            col1=c("#DCBCBC"), col2=c("#B97C7C")) {
  dens = density(x)
  plot(dens, main=main, xlab=xlab, ylab=ylab, xlim=xlim,
       lwd=lwd, yaxt='n', bty='n', type='n')
  polygon(dens, col=col1, border=NA)
  qbot <- quantile(x, bot)
  qtop <- quantile(x, top)
  x1 = min(which(dens$x >= qbot))
  x2 = max(which(dens$x < qtop))
  with(dens, polygon(x=x[c(x1,x1:x2,x2)], y=c(0, y[x1:x2], 0), col=col2, border=NA))
}
```

And now we can view the posteriors as compared to the true parameter values which were used to generate the data.

```{r}
# Plot true vs posterior for rho
posterior = extract(fit)
par(mfrow=c(3, 1))

#hist(posterior$rho, main="", xlab="rho", ylab="",
#     col=c_dark, yaxt='n', breaks=20)
interval_density(posterior$rho, xlab="rho")
abline(v=rho, col=c_dark, lwd=3)

#hist(posterior$alpha, main="", xlab="alpha", ylab="",
#     col=c_dark, yaxt='n', breaks=20)
interval_density(posterior$rho, xlab="alpha")
abline(v=alpha, col=c_dark, lwd=3)

#hist(posterior$sigma, main="", xlab="sigma", ylab="",
#     col=c_dark, yaxt='n', breaks=20)
interval_density(posterior$rho, xlab="sigma")
abline(v=sigma, col=c_dark, lwd=3)
```

The true parameter values are all within the bulk of the posterior, meaning we were able to successfully recover the true parameters!

## Hidden Markov Models

TODO: intro

### The Math

Our model will have two hidden states ($N=2$).  This means that our model has two free parameters which control the transition probabilities $\phi$.  The probability of transitioning from state $i$ to state $j$ is $\phi_{i,j}$.  In our Stan model, we'll fit the "recurrent" transition probabilities - that is, the probability that the hidden state at the current timestep will be the same as the last timestep.  So, the two free parameters which control the transition probabilities are:

$$
\phi_{i,i} ~ \text{for} ~ i \in \{1,2\}
$$

We model the probability of an observation given the state as a beta distribution.  However, for each hidden state, one parameter of the distribution is fixed at $1$, and the other parameter is free.  This gives us two additional parameters ($\theta_1$ and $\theta_2$) which control the observation probabilities corresponding to the hidden states.  So for the first state $x_1$,


$$
y|x_1 \sim \text{Beta}(1, \theta_1)
$$

and for the second state $x_2$,

$$
y|x_2 \sim \text{Beta}(\theta_2, 1)
$$

TODO: explain why and plot of the beta dist for each state

TODO: explain how you compute the prob marginalized over hidden states w/ the forward algorithm

### Priors

For the observation model parameters ($\theta$) we use a gamma prior with $\alpha,\beta=2$, and limit the values to be $\geq 1$,

$$
\theta_i - 1 \sim \text{Gamma}(\alpha=2, \beta=2)
$$

This results in a distribution which has zero density $<1$, and low density at higher values.

NOTE: could also use a log-Cauchy distribution ($\theta \sim 1/(log(x)^2+1)$)?  Same bounds, has ridiculously long tail on the positive end...

```{r}
theta_prior <- function(x) dgamma(x-1, 2, 1/2)
plot(theta_prior, 0, 10, xlab='Theta', ylab='',
     main="Observation model parameter prior",
      yaxt='n', bty='n', lwd=3, col=c_blue_mid)
```

(Side note: the notation above and the Stan programs below use the $\text{Gamma}(\alpha,\beta)$ parameterization of the gamma distribution, while the R function `dgamma` uses the $\text{Gamma}(\kappa,\theta)$ parameterization, where $\kappa=\alpha$ and $\theta=1/\beta$, in case you were wondering why the parameters in the call to `dgamma` were $2,1/2$ instead of $2,2$)

For the recurrent transition probability prior, we use a beta distribution with $\alpha,\beta=2$,

$$
\phi_{i,i} \sim \text{Beta}(2, 2)
$$

This results in a distribution which is bounded between $0$ and $1$, and has lower density towards more extreme values.

```{r}
trans_prior <- function(x) dbeta(x, 2, 2)
plot(trans_prior, 0, 1, xlab='Phi_{i,i}', ylab='',
     main="Recurrent transition probability prior",
     type='l', yaxt='n', bty='n', lwd=3, col=c_blue_mid)
```


### Generating Data from a Hidden Markov Model

TODO: talk about stan file to generate data

```{r}
writeLines(readLines("simulate_hmm.stan"))
```

We can use this Stan model to generate data from our hidden Markov model.  Let's generate 100 samples.

```{r}
# Data
N = 100
```

We will also set the parameters of the HMM.

```{r}
# Parameters
phi = array(c(0.8, 0.2, 0.2, 0.8), dim=c(2,2)) #transition probabilities
theta = c(5, 5) #observation distribution parameters
```

Now we can run the Stan model, which will generate data from a hidden Markov model with these parameters.

```{r}
# Simulate
sim_params = list(N=N, phi=phi, theta=theta)
sim_fit = stan(file='simulate_hmm.stan', data=sim_params, iter=1, 
               chains=1, seed=seed, algorithm="Fixed_param")

# Extract simulated output
s = t(extract(sim_fit)$s)-1
y = t(extract(sim_fit)$y)
sim_data = data.frame(x=seq(N), s=s, y=y)
```

Let's plot our generated Gaussian process:

```{r, dev='svg'}
# Plot generated data
ggplot(sim_data) +
  geom_line(aes(x=x, y=s), size=1) +
  geom_point(aes(x=x, y=y), color=c_mid) +
  ggtitle('Simulated Hidden Markov Model')
```

Notice how the latent function varies smoothly over time, and varies between 0 and 1.  The observations have more jitter than the latent function (because they are generated from that function but with added noise), but each point is in the neigborhood of the previous point, and they are also bounded between 0 and 1.  Because of the way we defined the observation part of the model, observations during the first hidden state are close to 0, and observations during the second hidden state are close to 1.  However, all observations are between 0 and 1, and there is again some noise to the observations.


### Fitting a Hidden Markov Model

Now that we've generated some data from a Gaussian process, we can attempt to fit a Gaussian process model to that data.  We'll use a separate Stan model, one which fits the parameters of the model to the data, instead of generating data from the parameters.

```{r}
writeLines(readLines("hmm.stan"))
```

Let's fit the model to our generated data.

```{r}
# Fit
y = y[,1]
stan_rdump(c("N", "y"), file="simulated_hmm.data.R")
fit_data = read_rdump("simulated_hmm.data.R")
fit = stan(file='hmm.stan', data=fit_data, seed=seed)
```

After MCMC sampling has finished, we can view the posterior intervals for each parameter.

```{r}
# Show results of fit
print(fit)
```

Again we'll check several diagnoistics to ensure our sampling effectively explored the parameter space.  The Rhat values look good, and if we look at the posterior distributions per chain, it appears the chains converged.


```{r}
# Plot density per chain
posterior = as.array(fit)
mcmc_dens_overlay(posterior, 
                  pars=c("phi[1,1]", "phi[2,2]", "theta[1]", "theta[2]"))
```

The trace plots also indicate that the chains (likely) explored similar areas of parameter space, and that the warmup period was (probably) long enough.

```{r}
# Plot trace per chain
mcmc_trace(posterior, 
           pars=c("phi[1,1]", "phi[2,2]", "theta[1]", "theta[2]"))
```

Our other three diagnostics also look good:

```{r}
# Check other diagnostics
check_treedepth(fit)
check_energy(fit)
check_divergences(fit)
```

Now we can see how the posterior distributions for the parameters match up to the true parameters we used to generate the data.  We'll also plot the prior distributions for each parameter so we can see how that effects the posterior.  The prior distribution is plotted in blue, the posterior distribution in red, and the parameter value used to generate the data as a dark red vertical line.

```{r}
# Plot true vs posterior for rho
posterior = extract(fit)
par(mfrow=c(2, 2))

beta_prior <- function(x) dbeta(x, 2, 2)
gamma_prior <- function(x) dgamma(x, 2, 1/2)

interval_density(posterior$phi[,1,1], xlab="phi[,1,1]", xlim=c(0,1))
plot(beta_prior, 0, 1, add=TRUE, col=c_blue_light, lwd=3)
abline(v=phi[1,1], col=c_dark, lwd=3)

interval_density(posterior$phi[,2,2], xlab="phi[,2,2]", xlim=c(0,1))
plot(beta_prior, 0, 1, add=TRUE, col=c_blue_light, lwd=3)
abline(v=phi[2,2], col=c_dark, lwd=3)

interval_density(posterior$theta[,1], xlab="theta[1]", xlim=c(0,12))
plot(gamma_prior, 0, 12, add=TRUE, col=c_blue_light, lwd=3)
abline(v=theta[1], col=c_dark, lwd=3)

interval_density(posterior$theta[,2], xlab="theta[2]", xlim=c(0,12))
plot(gamma_prior, 0, 12, add=TRUE, col=c_blue_light, lwd=3)
abline(v=theta[2], col=c_dark, lwd=3)
```

We were mostly able to recover the parameters which generated the data - but not perfectly!  This is because we set parameter values which didn't align very well with the priors - with more data the posteriors would be pulled even closer to the true vaules.  Notice how the posteriors for the first state's parameters ($\phi_{1,1}$ and $\theta_1$) are closer to the true value than the posteriors for the second state's parameters.  This is because in our generated data, more observations occurred during the first state.  Therefore, the fit was able to better infer the true parameters corresponding to that state.  Our posteriors were indeed pulled towards the true parameter values, which is what we want to see, and indicates our Bayesian fit of the hidden Markov model was successful - we'd just ideally have more data!


## Bridge Sampling and Model Comparison

TODO
